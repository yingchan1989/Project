{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Twitter Trades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group: Individual (Ying Chan)\n",
    "\n",
    "\n",
    "Data Sources:\n",
    "The data source will be Twitter tweets API in general. We will combine this data set with the stock price data obtained preferrably by the minute or 5-minute intervals. The stock return will be calculated and tagged against tweets.\n",
    "\n",
    "Objective:\n",
    "The ultimate objective is to create a natural language processing algorithm which will create feature extractions from tweets that help explain siginificant movement in stock prices. For example, certain tweets such as \"price gouging\" might affect a pharmaceutical drug stock. \n",
    "\n",
    "Evaluate:\n",
    "The evaluation technique for this type of model is quite high given the nature of the project, which is to see if certain tweets will be able to predict future moves that are significant. For example, if a tweet relating to \"drug price\" against this stock (or even other stocks) that causes the model to predict a movement does indeed predict a movement. The magnitude of movement is not an important measure here, though the direction is important. Another metric that can be measured will be a model portfolio performance using this algorithm, whether it will \"beat\" a traditional risk free rate return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tweepy library and start collecting search terms based on stocks in the S&P500. We start with a specific stock $RDUS for testing purposes of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "\n",
    "consumer_key=\"LX38iNoplbbMid20arGtsivQh\"\n",
    "consumer_secret=\"w1KQHobzm77FFdVMHIzHJJPPaPrHdx53e2Dq8CnuOwh0yr7luD\"\n",
    "\n",
    "access_token=\"268119650-HPK3lM3WSCnDAvl8d7PDmxwcWcRDC1XUZYr77doh\"\n",
    "access_token_secret=\"tu4UNdMdCWh0E7AuduS3EI2xliPSUv1TXTqPCj3NuglFb\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "results = api.search(q=\"$RDUS\", count=1000 , include_entities=True, lang=\"en\", since_id=2016-10-01)\n",
    "\n",
    "with open('tweets.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for result in results:\n",
    "        writer.writerow([result.created_at, unicode(result.text).encode(\"utf-8\") ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create process tweet function to make all the words lower case and convert any hastags into normal words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Perform basic pre-processing of tweet to improve our NLP model\n",
    "def preprocess_tweet(tweet):\n",
    "    #Lower case all words\n",
    "    tweet = tweet.lower()\n",
    "    #Remove URL strings\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Remove double spacing\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Remove usernames\n",
    "    tweet = re.sub('@[^\\s]+','TWITTER_USER_NAME',tweet)\n",
    "    #Remove hashtags\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #strip punctuation\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV data from tweets and stocks to merge and append to dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-075c5f035c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PreviousClose'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mstocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreviousClose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yingchan/Library/Python/2.7/lib/python/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m    713\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "tweets = pandas.read_csv('tweets.csv', header=None)\n",
    "stocks = pandas.read_csv('rdus.us.txt')\n",
    "\n",
    "tweets.columns = ['timestamp', 'tweet']\n",
    "\n",
    "stocks['Timestamp'] = stocks.Date.astype(str).str.cat(stocks.Time.astype(str), sep=' ')\n",
    "\n",
    "#Convert to pandas date time format\n",
    "stocks['Timestamp'] = pd.to_datetime(stocks['Timestamp']) \n",
    "tweets['timestamp'] = pd.to_datetime(tweets['timestamp']) \n",
    "\n",
    "stocks['PreviousClose'] = stocks.Close.shift(1)\n",
    "stocks['Return'] = (stocks.Close/stocks.PreviousClose)-1\n",
    "\n",
    "print stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to find the closest date and apply to existing panda dataframes to join the tweets data to the stock data. We keep the tweets data as the left dataframe as this is the data that is of interest. If there is no tweet, there is no need to observe stock data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp                                              tweet  \\\n",
      "0  2016-11-17 04:03:33  $RDUS Company Radius Health, Inc. just submite...   \n",
      "1  2016-11-16 23:16:11  The seasonality trend for $RDUS might surprise...   \n",
      "2  2016-11-16 17:28:27  Radius Health Inc. $RDUS Stake Boosted by Jenn...   \n",
      "3  2016-11-16 08:04:25  Radius Health Inc. $RDUS Receives Average Reco...   \n",
      "4  2016-11-15 22:35:19  $ALBO $RDUS $CPHI $STRP $ON \\n\\nBreakout/Algor...   \n",
      "5  2016-11-15 21:22:58  RT @d_voted_1: $RDUS Can we take a breather?  ...   \n",
      "6  2016-11-15 19:20:21  $RDUS Can we take a breather?  Incredible rebo...   \n",
      "7  2016-11-15 18:34:47  $RDUS gorgeous Daily 200sma rotation back over...   \n",
      "8  2016-11-15 17:53:19  RT @biotechnova: Perceptive New Buys: $CLCD $R...   \n",
      "9  2016-11-15 17:50:54  RT @biotechnova: Perceptive New Buys: $CLCD $R...   \n",
      "10 2016-11-15 17:49:34  Perceptive New Buys: $CLCD $RDUS $FLML $AERI, ...   \n",
      "11 2016-11-15 17:48:56  @Lblegend33 @NguyenTu0319 @Steam_Sports @thefl...   \n",
      "12 2016-11-15 17:46:24  $RDUS good amount of bullish calls again, not ...   \n",
      "13 2016-11-15 14:22:58  Option trading signal alerts with over 70% acc...   \n",
      "14 2016-11-15 06:58:08  Q3 13F highlights: Steve had/has 780K sh of $O...   \n",
      "15 2016-11-15 06:43:43  $dia $ge $jd $dis $ftnw $googl $FB $GalT $nke ...   \n",
      "16 2016-11-15 06:34:20  RT @microcapcompany: Checking out some #stocks...   \n",
      "17 2016-11-15 06:34:20  RT @microcapcompany: Checking out some #stocks...   \n",
      "18 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "19 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "20 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "21 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "22 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "23 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "24 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "25 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "26 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "27 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "28 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "29 2016-11-15 06:34:19  RT @microcapcompany: Checking out some #stocks...   \n",
      "..                 ...                                                ...   \n",
      "55 2016-11-11 15:42:38  RT @AndyBiotech: EMA CHMP issues positive opin...   \n",
      "56 2016-11-11 15:16:23  RT @AndyBiotech: EMA CHMP issues positive opin...   \n",
      "57 2016-11-11 14:59:53  RT @AndyBiotech: EMA CHMP issues positive opin...   \n",
      "58 2016-11-11 14:59:24  EMA CHMP issues positive opinions on two #bios...   \n",
      "59 2016-11-11 14:46:29     $RDUS $RIGL won't stop https://t.co/MpLeefUNRg   \n",
      "60 2016-11-11 14:14:03  RT @SheffStation: $MACK..interesting even thou...   \n",
      "61 2016-11-11 13:58:11  CANT $RDUS Radius Health, Inc.: Closer to Comm...   \n",
      "62 2016-11-11 13:44:03  If you follow $BLL This is a must join site! -...   \n",
      "63 2016-11-11 13:28:19  $MACK..interesting even though I think I have ...   \n",
      "64 2016-11-10 22:19:49  @fascination_bio $rdus awesome reversal right ...   \n",
      "65 2016-11-10 22:18:30  $FOSL $RDUS $CONE $BTI \\n\\n Use Algorithmic Bu...   \n",
      "66 2016-11-10 21:09:24  Chaser charts ( overbought) on watch for tomor...   \n",
      "67 2016-11-10 20:57:27  RT @gearle2: Come on $ibb let's get over 290 a...   \n",
      "68 2016-11-10 20:55:39  $BSMX $CECO $RDUS $UBSI \\n\\n Use Algorithmic B...   \n",
      "69 2016-11-10 19:51:35  $RDUS back over 50. thinking if shire wanted i...   \n",
      "70 2016-11-10 19:40:37  $RDUS $HQY $CF $MOMO $MEOH $REN $ITG $AMG $END...   \n",
      "71 2016-11-10 19:23:31  Come on $ibb let's get over 290 and not look b...   \n",
      "72 2016-11-10 17:56:04  runnin' runnin' and runnin' runnin' and runnin...   \n",
      "73 2016-11-10 17:17:14                                      $RDUS new HOD   \n",
      "74 2016-11-10 16:11:15                            $Rdus almost back to 50   \n",
      "75 2016-11-10 14:04:44  Finished reading $RDUS CC https://t.co/Yk7NySa...   \n",
      "76 2016-11-10 10:39:04  If you follow $GPRO Check out this site! ----&...   \n",
      "77 2016-11-10 00:17:41  $CRL $OSIR $UIS $RDUS $WAB \\n\\nhttps://t.co/Py...   \n",
      "78 2016-11-09 23:37:05  If you follow $BSQR What do you think about it...   \n",
      "79 2016-11-09 20:45:17  $CAH $EPE $FET $AEG $RDUS $AERI $HZNP $SCMP $O...   \n",
      "80 2016-11-09 20:35:00  Upcoming PDUFAs still time to see bull action,...   \n",
      "81 2016-11-09 19:47:35  $XBI $IBB GapUp, Short Squish Mosh Pit $LABU $...   \n",
      "82 2016-11-09 17:50:05  If you follow $GCP What do you think about it?...   \n",
      "83 2016-11-08 16:27:04  Radius Health’s $RDUS “Buy” Rating Reiterated ...   \n",
      "84 2016-11-08 14:29:39  How to profit on weed microcap stocks. Read: h...   \n",
      "\n",
      "     closest_timestamp     diff        Date      Time    Open     High  \\\n",
      "0  2016-11-16 22:00:00 06:03:33  2016-11-16  22:00:00  52.450  52.5200   \n",
      "1  2016-11-16 22:00:00 01:16:11  2016-11-16  22:00:00  52.450  52.5200   \n",
      "2  2016-11-16 17:30:00 00:01:33  2016-11-16  17:30:00  54.420  54.6400   \n",
      "3  2016-11-16 15:35:00 07:30:35  2016-11-16  15:35:00  54.850  54.8900   \n",
      "4  2016-11-15 22:00:00 00:35:19  2016-11-15  22:00:00  54.900  54.9300   \n",
      "5  2016-11-15 21:25:00 00:02:02  2016-11-15  21:25:00  54.990  55.1900   \n",
      "6  2016-11-15 19:20:00 00:00:21  2016-11-15  19:20:00  54.990  55.0900   \n",
      "7  2016-11-15 18:35:00 00:00:13  2016-11-15  18:35:00  54.790  54.9200   \n",
      "8  2016-11-15 17:55:00 00:01:41  2016-11-15  17:55:00  54.660  54.8000   \n",
      "9  2016-11-15 17:50:00 00:00:54  2016-11-15  17:50:00  54.360  54.6300   \n",
      "10 2016-11-15 17:50:00 00:00:26  2016-11-15  17:50:00  54.360  54.6300   \n",
      "11 2016-11-15 17:50:00 00:01:04  2016-11-15  17:50:00  54.360  54.6300   \n",
      "12 2016-11-15 17:45:00 00:01:24  2016-11-15  17:45:00  54.345  54.3700   \n",
      "13 2016-11-15 15:35:00 01:12:02  2016-11-15  15:35:00  54.060  54.0600   \n",
      "14 2016-11-15 15:35:00 08:36:52  2016-11-15  15:35:00  54.060  54.0600   \n",
      "15 2016-11-14 22:00:00 08:43:43  2016-11-14  22:00:00  54.040  54.4000   \n",
      "16 2016-11-14 22:00:00 08:34:20  2016-11-14  22:00:00  54.040  54.4000   \n",
      "17 2016-11-14 22:00:00 08:34:20  2016-11-14  22:00:00  54.040  54.4000   \n",
      "18 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "19 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "20 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "21 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "22 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "23 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "24 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "25 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "26 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "27 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "28 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "29 2016-11-14 22:00:00 08:34:19  2016-11-14  22:00:00  54.040  54.4000   \n",
      "..                 ...      ...         ...       ...     ...      ...   \n",
      "55 2016-11-11 15:45:00 00:02:22  2016-11-11  15:45:00  51.060  52.2700   \n",
      "56 2016-11-11 15:35:00 00:18:37  2016-11-11  15:35:00  50.070  50.7900   \n",
      "57 2016-11-11 15:35:00 00:35:07  2016-11-11  15:35:00  50.070  50.7900   \n",
      "58 2016-11-11 15:35:00 00:35:36  2016-11-11  15:35:00  50.070  50.7900   \n",
      "59 2016-11-11 15:35:00 00:48:31  2016-11-11  15:35:00  50.070  50.7900   \n",
      "60 2016-11-11 15:35:00 01:20:57  2016-11-11  15:35:00  50.070  50.7900   \n",
      "61 2016-11-11 15:35:00 01:36:49  2016-11-11  15:35:00  50.070  50.7900   \n",
      "62 2016-11-11 15:35:00 01:50:57  2016-11-11  15:35:00  50.070  50.7900   \n",
      "63 2016-11-11 15:35:00 02:06:41  2016-11-11  15:35:00  50.070  50.7900   \n",
      "64 2016-11-10 22:00:00 00:19:49  2016-11-10  22:00:00  51.240  51.8100   \n",
      "65 2016-11-10 22:00:00 00:18:30  2016-11-10  22:00:00  51.240  51.8100   \n",
      "66 2016-11-10 21:10:00 00:00:36  2016-11-10  21:10:00  50.460  50.5000   \n",
      "67 2016-11-10 20:55:00 00:02:27  2016-11-10  20:55:00  50.110  50.4500   \n",
      "68 2016-11-10 20:55:00 00:00:39  2016-11-10  20:55:00  50.110  50.4500   \n",
      "69 2016-11-10 19:50:00 00:01:35  2016-11-10  19:50:00  49.470  49.7800   \n",
      "70 2016-11-10 19:40:00 00:00:37  2016-11-10  19:40:00  49.530  49.5600   \n",
      "71 2016-11-10 19:25:00 00:01:29  2016-11-10  19:25:00  49.770  49.9700   \n",
      "72 2016-11-10 17:55:00 00:01:04  2016-11-10  17:55:00  48.890  49.1900   \n",
      "73 2016-11-10 17:15:00 00:02:14  2016-11-10  17:15:00  48.460  48.9600   \n",
      "74 2016-11-10 16:10:00 00:01:15  2016-11-10  16:10:00  48.480  48.7800   \n",
      "75 2016-11-10 15:35:00 01:30:16  2016-11-10  15:35:00  48.000  48.4118   \n",
      "76 2016-11-10 15:35:00 04:55:56  2016-11-10  15:35:00  48.000  48.4118   \n",
      "77 2016-11-09 22:00:00 02:17:41  2016-11-09  22:00:00  47.060  47.0800   \n",
      "78 2016-11-09 22:00:00 01:37:05  2016-11-09  22:00:00  47.060  47.0800   \n",
      "79 2016-11-09 20:45:00 00:00:17  2016-11-09  20:45:00  47.120  47.3600   \n",
      "80 2016-11-09 20:35:00 00:00:00  2016-11-09  20:35:00  46.510  47.0500   \n",
      "81 2016-11-09 19:50:00 00:02:25  2016-11-09  19:50:00  46.320  46.4000   \n",
      "82 2016-11-09 17:50:00 00:00:05  2016-11-09  17:50:00  46.040  46.2500   \n",
      "83 2016-11-08 16:25:00 00:02:04  2016-11-08  16:25:00  40.840  40.8500   \n",
      "84 2016-11-08 15:35:00 01:05:21  2016-11-08  15:35:00  41.670  42.1300   \n",
      "\n",
      "        Low  Close  Volume  OpenInt           Timestamp  PreviousClose  \\\n",
      "0   52.2975  52.48   78830        0 2016-11-16 22:00:00         52.480   \n",
      "1   52.2975  52.48   78830        0 2016-11-16 22:00:00         52.480   \n",
      "2   54.3300  54.64    3700        0 2016-11-16 17:30:00         54.310   \n",
      "3   54.1500  54.89   12101        0 2016-11-16 15:35:00         54.900   \n",
      "4   54.6800  54.90   71071        0 2016-11-15 22:00:00         54.900   \n",
      "5   54.9700  55.10    3600        0 2016-11-15 21:25:00         55.010   \n",
      "6   54.9400  55.09   11792        0 2016-11-15 19:20:00         55.020   \n",
      "7   54.6850  54.92    1500        0 2016-11-15 18:35:00         54.770   \n",
      "8   54.5600  54.66    5165        0 2016-11-15 17:55:00         54.630   \n",
      "9   54.3600  54.63    5067        0 2016-11-15 17:50:00         54.320   \n",
      "10  54.3600  54.63    5067        0 2016-11-15 17:50:00         54.320   \n",
      "11  54.3600  54.63    5067        0 2016-11-15 17:50:00         54.320   \n",
      "12  54.3200  54.32    1300        0 2016-11-15 17:45:00         54.380   \n",
      "13  53.2200  53.22    8873        0 2016-11-15 15:35:00         54.270   \n",
      "14  53.2200  53.22    8873        0 2016-11-15 15:35:00         54.270   \n",
      "15  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "16  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "17  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "18  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "19  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "20  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "21  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "22  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "23  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "24  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "25  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "26  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "27  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "28  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "29  54.0400  54.27   92801        0 2016-11-14 22:00:00         54.100   \n",
      "..      ...    ...     ...      ...                 ...            ...   \n",
      "55  50.9900  52.16   15494        0 2016-11-11 15:45:00         51.075   \n",
      "56  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "57  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "58  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "59  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "60  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "61  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "62  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "63  50.0000  50.41   13647        0 2016-11-11 15:35:00         51.080   \n",
      "64  50.9200  51.08  185468        0 2016-11-10 22:00:00         51.230   \n",
      "65  50.9200  51.08  185468        0 2016-11-10 22:00:00         51.230   \n",
      "66  50.3500  50.45    5365        0 2016-11-10 21:10:00         50.490   \n",
      "67  50.1100  50.36    8012        0 2016-11-10 20:55:00         50.060   \n",
      "68  50.1100  50.36    8012        0 2016-11-10 20:55:00         50.060   \n",
      "69  49.4700  49.65   15701        0 2016-11-10 19:50:00         49.570   \n",
      "70  49.3200  49.56    5504        0 2016-11-10 19:40:00         49.480   \n",
      "71  49.5000  49.60    9514        0 2016-11-10 19:25:00         49.700   \n",
      "72  48.8100  48.99    9500        0 2016-11-10 17:55:00         48.850   \n",
      "73  48.2500  48.95   12741        0 2016-11-10 17:15:00         48.580   \n",
      "74  48.2800  48.43   10452        0 2016-11-10 16:10:00         48.260   \n",
      "75  46.5900  47.99   21631        0 2016-11-10 15:35:00         46.860   \n",
      "76  46.5900  47.99   21631        0 2016-11-10 15:35:00         46.860   \n",
      "77  46.7700  46.86  147841        0 2016-11-09 22:00:00         47.035   \n",
      "78  46.7700  46.86  147841        0 2016-11-09 22:00:00         47.035   \n",
      "79  47.1100  47.31    8293        0 2016-11-09 20:45:00         47.130   \n",
      "80  46.5100  46.89    9566        0 2016-11-09 20:35:00         46.460   \n",
      "81  46.1100  46.30   22549        0 2016-11-09 19:50:00         46.360   \n",
      "82  46.0400  46.10    8000        0 2016-11-09 17:50:00         46.000   \n",
      "83  40.5700  40.57   11312        0 2016-11-08 16:25:00         40.790   \n",
      "84  41.5000  41.90   31352        0 2016-11-08 15:35:00         42.000   \n",
      "\n",
      "      Return  \n",
      "0   0.000000  \n",
      "1   0.000000  \n",
      "2   0.006076  \n",
      "3  -0.000182  \n",
      "4   0.000000  \n",
      "5   0.001636  \n",
      "6   0.001272  \n",
      "7   0.002739  \n",
      "8   0.000549  \n",
      "9   0.005707  \n",
      "10  0.005707  \n",
      "11  0.005707  \n",
      "12 -0.001103  \n",
      "13 -0.019348  \n",
      "14 -0.019348  \n",
      "15  0.003142  \n",
      "16  0.003142  \n",
      "17  0.003142  \n",
      "18  0.003142  \n",
      "19  0.003142  \n",
      "20  0.003142  \n",
      "21  0.003142  \n",
      "22  0.003142  \n",
      "23  0.003142  \n",
      "24  0.003142  \n",
      "25  0.003142  \n",
      "26  0.003142  \n",
      "27  0.003142  \n",
      "28  0.003142  \n",
      "29  0.003142  \n",
      "..       ...  \n",
      "55  0.021243  \n",
      "56 -0.013117  \n",
      "57 -0.013117  \n",
      "58 -0.013117  \n",
      "59 -0.013117  \n",
      "60 -0.013117  \n",
      "61 -0.013117  \n",
      "62 -0.013117  \n",
      "63 -0.013117  \n",
      "64 -0.002928  \n",
      "65 -0.002928  \n",
      "66 -0.000792  \n",
      "67  0.005993  \n",
      "68  0.005993  \n",
      "69  0.001614  \n",
      "70  0.001617  \n",
      "71 -0.002012  \n",
      "72  0.002866  \n",
      "73  0.007616  \n",
      "74  0.003523  \n",
      "75  0.024114  \n",
      "76  0.024114  \n",
      "77 -0.003721  \n",
      "78 -0.003721  \n",
      "79  0.003819  \n",
      "80  0.009255  \n",
      "81 -0.001294  \n",
      "82  0.002174  \n",
      "83 -0.005393  \n",
      "84 -0.002381  \n",
      "\n",
      "[85 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "def find_closest_date(timepoint, time_series, add_time_delta_column=True):\n",
    "    deltas = np.abs(time_series - timepoint)\n",
    "    idx_closest_date = np.argmin(deltas)\n",
    "    res = {\"closest_date\": time_series.ix[idx_closest_date]}\n",
    "    idx = ['closest_date']\n",
    "    if add_time_delta_column:\n",
    "        res[\"closest_delta\"] = deltas[idx_closest_date]\n",
    "        idx.append('closest_delta')\n",
    "    return pd.Series(res, index=idx)\n",
    "\n",
    "\n",
    "tweets[['closest_timestamp', 'diff']] = tweets.timestamp.apply(\n",
    "                                          find_closest_date, args=[stocks.Timestamp])\n",
    "\n",
    "combined = pd.merge(tweets, stocks, left_on=['closest_timestamp'], right_on=['Timestamp'])\n",
    "\n",
    "print combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create feature list using function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 85\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(combined.tweet.tolist())\n",
    "\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'abaloparatid', u'abx', u'acad', u'aeri', u'afternoon', u'alert', u'algorithm', u'amp', u'amzn', u'analyst', u'andybiotech', u'averag', u'axon', u'befor', u'bet', u'biib', u'biosimilar', u'buy', u'check', u'chmp', u'clcd', u'compani', u'dec', u'depo', u'di', u'dia', u'eem', u'ema', u'etsi', u'fb', u'flml', u'follow', u'forteo', u'friday', u'ftnw', u'galt', u'ge', u'googl', u'gt', u'gwph', u'health', u'http', u'ibb', u'impact', u'issu', u'jd', u'leak', u'mack', u'market', u'microcapcompani', u'mkt', u'mux', u'n', u'nbix', u'new', u'nke', u'open', u'opinion', u'oxi', u'place', u'posit', u'radiu', u'rdu', u'read', u'receiv', u'recommend', u'rt', u'runnin', u's', u'sell', u'signal', u'size', u'sleeper', u'sold', u'spb', u'stock', u't', u'thi', u'think', u'tomorrow', u'trade', u'tsla', u'w', u'x', u'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 abaloparatid\n",
      "3 abx\n",
      "3 acad\n",
      "4 aeri\n",
      "3 afternoon\n",
      "6 alert\n",
      "5 algorithm\n",
      "3 amp\n",
      "20 amzn\n",
      "5 analyst\n",
      "6 andybiotech\n",
      "3 averag\n",
      "3 axon\n",
      "20 befor\n",
      "3 bet\n",
      "3 biib\n",
      "7 biosimilar\n",
      "16 buy\n",
      "20 check\n",
      "7 chmp\n",
      "3 clcd\n",
      "3 compani\n",
      "3 dec\n",
      "3 depo\n",
      "20 di\n",
      "20 dia\n",
      "3 eem\n",
      "7 ema\n",
      "3 etsi\n",
      "20 fb\n",
      "3 flml\n",
      "4 follow\n",
      "7 forteo\n",
      "3 friday\n",
      "20 ftnw\n",
      "20 galt\n",
      "20 ge\n",
      "20 googl\n",
      "4 gt\n",
      "4 gwph\n",
      "14 health\n",
      "49 http\n",
      "3 ibb\n",
      "7 impact\n",
      "7 issu\n",
      "20 jd\n",
      "3 leak\n",
      "3 mack\n",
      "19 market\n",
      "18 microcapcompani\n",
      "7 mkt\n",
      "3 mux\n",
      "18 n\n",
      "3 nbix\n",
      "5 new\n",
      "20 nke\n",
      "19 open\n",
      "7 opinion\n",
      "3 oxi\n",
      "3 place\n",
      "8 posit\n",
      "14 radiu\n",
      "66 rdu\n",
      "6 read\n",
      "3 receiv\n",
      "3 recommend\n",
      "31 rt\n",
      "10 runnin\n",
      "12 s\n",
      "6 sell\n",
      "3 signal\n",
      "7 size\n",
      "3 sleeper\n",
      "3 sold\n",
      "3 spb\n",
      "23 stock\n",
      "44 t\n",
      "8 thi\n",
      "5 think\n",
      "20 tomorrow\n",
      "3 trade\n",
      "20 tsla\n",
      "3 w\n",
      "3 x\n",
      "4 z\n"
     ]
    }
   ],
   "source": [
    "dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        corpus_data_features_nd[0:len(combined)], \n",
    "        combined.Return,\n",
    "        train_size=0.85)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0059928086296443528, 0.0031423290203327348, -0.0068130204390614013, 0.0031423290203327348, 0.0073786407766991413, -0.00079223608635370368, 0.0031423290203327348, 0.0, -0.0037206335707451466, 0.0019508388607101246, -0.013116679718089275, -0.013116679718089275, 0.0050260970423352536]\n",
      "[-0.00292797  0.00314233 -0.00681302  0.00314233 -0.01284263 -0.00292797\n",
      "  0.00314233 -0.00292797  0.02411438 -0.00236005  0.02411438 -0.00292797\n",
      " -0.00018215]\n"
     ]
    }
   ],
   "source": [
    "y_test_final = pd.Series.tolist(y_test)\n",
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "print y_test_final\n",
    "print y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
